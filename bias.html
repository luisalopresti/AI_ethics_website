<!DOCTYPE html>
<html>
<head>
    <meta name='viewport' content='with=device-width, initial-scale=1.0'> <!--responsiveness-->
    <title>AI & HUMAN RIGHTS</title>
    <link rel="icon" href="images/logo.png">
    <link rel='stylesheet' href='style.css'>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montagu+Slab:wght@400;500;600&display=swap" rel="stylesheet">
    <!--add this to use icons-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body>
    <section class='SubHeader'>
        <nav> <!--major block of navigation link upper right corner--> 
            <a href='index.html'><img src='images/logo.png'></a>
            <div class='nav-links' id='navlinks'>
                <!--close bottom icon-->
                <i class="fa fa-times" onclick='hideMenu()'></i> <!--from https://fontawesome.com/v4.7/icons/ for icons-->
                <ul>
                    <li><a href='index.html'>HOME</a></li>
                    <li><a href='pag2_videos.html'>ETHICS</a></li>
                    <li><a href='bias.html'>BIASES & MINORITIES</a></li>
                    <li><a href='contacts.html'>CONTACTS</a></li>
                </ul>
            </div>
            <!--menu icon-->
            <i class="fa fa-bars" onclick='showMenu()'></i>
        </nav>
        <h1>Biases and Ethics</h1>
    </section>
  
   <!--written production-->
   <section class='bias'>
       <div class='row'>
            <div class='bias-left'>
                <h2>Which social groups would be more affected by AI applications?</h2>
                <p>Often, people view Artificial Intelligence as a matter concerning possible future developments in technology, but this underestimates the level of advancement achieved by now. Indeed, AI is already present in several aspects of our lives, nevertheless there is still a dangerous lack of regulation concerning the field. Machine learning and artificial intelligence can have the great power and merit of enhancing various aspects in our work and daily life, but they can also be used in a variety of malicious and error-prone ways. The main concern that must be pointed out is the possible replication and extension of discriminations and unfair treatments already present in our societies, and the higher risks for minority groups. Given the fast development of the field, this threat leads to the urgent necessity of a regulatory framework within the European Union, and possibly some agreement on an international level.</p>
                <br>
                <p>When talking about AI and algorithms, we often expect a technology empowered with great performances, able to be unbiased and solve modern issues in a highly sophisticated way, hardly achievable by humans. This phenomenon is sometimes referred to as tech-washing, or the automation bias phenomenon. However, AI and machine learning models are trained with data coming from the real world, and it is undeniable that our societies are biased and happen to perpetuate unfair treatments and make unjust decisions. When training an algorithm, past data are used, and AI takes decisions and makes predictions based on this history.
                    Given the biases in our own decisions and data, we cannot expect AI to take better decisions, rather to highlight the unfairness present in our society, making them more noticeable and undeniable, or, in the worst-case scenario, to perpetuate and amplify them. A plausible risk, for example, is that AI could perpetuate the vision of certain jobs as peculiar to a particular gender, using this feature as a factor to base its decisions on.
                    This issue is not just a matter of not having enough data representing certain scenarios. Data are a picture of the society we are in, and a biased society can only lead to a biased AI. Thus, even in case we were able to collect more data and add diversity to the training datasets, we would still bring past biased decisions in the process, and this would lead, for example, to privilege men over women, or white people over people of color.</p>
                <br>
                <p>Moreover, algorithms design choices are also to be taken into account, as the selection of certain parameters or features may favor some categories of people with respect to others. This has to do with the concept of equity and equality, as given personal characteristics or discriminatory past, some people may have less access to resources than others. Thus, selecting these resources as feature would indirectly lead to an unfair mechanism of decision-making. In this framework, particular attention should be given to minorities and people at risk of discrimination, (i.e., women, people of color, elderly people, people belonging to the LGBT+ community, poor people, etc.), as automatic decision-making can be more harmful for them. A striking case that woken up the conscience of people of the UK on this topic, was the usage, of the Ofqual algorithm to assign a grade to students, given that the official examination could not be taken due to the Covid-19 pandemic. It turned out that the algorithm discriminated people coming from poorer backgrounds, assigning them a lower score, while people from private schools, for example, were advantaged in the evaluation process. It is relevant to notice that results such as this one have high impact on people’s future prospective, and unjust algorithms may be particularly damaging to one’s life.</p>
                <br>
                <p>Once AI and automatic decision-making have been demystified, it is easy to realize the possible issues that may arise in an unregulated context. There is not only the need to add diversity in the system, but also the necessity to build an accountability structure inside the tech industry and, now more than ever, to address society issues and inequality causes at the root.
                    Recently, at a European level, a new proposal dealing with the need of an accountability system and regulatory framework in AI has been advanced. This means could be a great tool for harmonizing legislations and ensuring a working and fairer system within the EU, following the footprint of GDPR in the context of personal data protection. Nevertheless, this European act on AI has been criticized by experts for its lack of coverage of several aspects that would harm the security and privacy of the citizens. In particular, this argument has been raised in the context of facial recognition, a very invasive tool, which in the proposal of the regulation seems to have several degrees of freedom in its usage. However, the act seems to take into account the two main fields of risks of AI: the danger of malfunctioning, which would lead to a threat to the physical safety of people, and the risk of perpetuating discriminations or lacking fairness in the decision-making process, particularly in areas that have a great impact on people’s lives and future prospective, such as education, employment, and loan eligibility.
                </p>
                <br>
                <p>
                    Yet, as mentioned, it would be beneficial to have not only a uniform and working legal framework within the EU, but also a broader agreement on an international level. Indeed, several countries, in particular the US and China, have invested great resources in the development of more and more advanced AI systems, as it is clear that a leading role in AI would ensure them great economic benefits for a long time.
                    Thus, alongside with the risk of unethical usage of AI, there is also the plausible fear that the AI race could lead to a conflict between China and the US.
                    Concerning the first one, several issues can be raised: in the Chinese context, for example, AI application in the Xinjiang region seems to be particularly concerning. Among other unethical practices, AI is employed to detect possible criminals, terrorists, or separatists, basing its decisions on the emotional state of the subject, which in a context of detention and interrogation is likely to be altered. In this setting, AI is used to predict future criminal behaviors of people, with the purpose of confirming authority’s prejudices without any real evidence. After this example, it comes without saying that the use of such technologies in a non-democratic context is more likely to lead to catastrophic results, thus an international treaty, involving as many parties as possible, and putting some constraints to the indiscriminate use of AI, is desirable. This could be done via the UN, which already expressed the fear of an escalation and of an AI arm race. Indeed, it is no surprise that one of the main fields of application of AI is the military, and the US is known to already have very advanced and autonomous AI weapons, able to target people from great distances.
                </p>
                <br>
                <p>The AI topic is becoming more and more a social, geo-politic, and military issue, and a delay in the implementation of a secure regulatory framework would increase the possible harm on minorities; acting against this scenario should be a priority in the international agenda, as urge a clear set of rules for monitoring the AI compliance with fundamental and human rights and with the non-harm of its use.</p>
            </div>
            <div class='bias-right'>
                <h3>References:</h3>
                <i><a href='https://www.nytimes.com/2021/07/30/opinion/artificial-intelligence-european-union.html?searchResultPosition=1#after-story-ad-1' target="_blank" rel="noopener noreferrer"><li>If You Don't Trust AI Yet, You're Not Wrong <br>- Frank Pasquale and Gianclaudio Malgieri</li></a></i>
                <i><a href='https://www.agendadigitale.eu/cultura-digitale/regolamento-ue-sullintelligenza-artificiale-tre-aspetti-da-approfondire/Regolamento' target="_blank" rel="noopener noreferrer"><li>EU Regulation on AI <br>- Ivana Bartoletti</li></a></i>
                <i><a href='https://www.huffingtonpost.co.uk/entry/a-level-algorithm_uk_5f436afac5b6305f3259aaec' target="_blank" rel="noopener noreferrer"><li>2020 Is The Year Britain Woke Up To The Politics Of Data <br>- Ivana Bartoletti</li></a></i>
                <i><a href='https://www.youtube.com/watch?v=67sbLrJAflQ' target="_blank" rel="noopener noreferrer"><li>The gender dimension in research <br>- European Commission</li></a></i>
            </div>
       </div>
   </section>

   <!--footer-->
   <section class='footer'>
       <h4>credits</h4>
       <p>The content of this website has been elaborated in the context of the seminar and workshop of <i>"Artificial Intelligence and Human Rights"</i>, <br>held at the Department of Sociology and Social Research - University of Trento (2021/2022).</p>
       <p>Designed and written by Luisa Lo Presti</p>
       <!--social media links-->
       <div class='icons'>
           <a href='https://www.linkedin.com/in/luisa-lo-presti-629747213/'><i class='fa fa-linkedin'></i></a>
           <a href='https://github.com/luisalopresti'><i class="fa fa-github" aria-hidden="true"></i></a>
       </div>
   </section>


<!--JavaScript-->
<script>
    /* JavaScript to open & close the drop down menu on mobiles*/
    var navlinks = document.getElementById('navlinks');
    function showMenu() {
        navlinks.style.right = '0';
    }
    function hideMenu() {
        navlinks.style.right = '-200px';
    }
</script>
</body>
</html>